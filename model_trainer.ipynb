{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy matplotlib seaborn torch sklearn transformers optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# configurations\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "# device for GPU usage if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'q_quora.csv'\n",
    "df = pd.read_csv(data_path, usecols=['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate'])\n",
    "\n",
    "# handling missing values by removing rows with missing questions\n",
    "df.dropna(subset=['question1', 'question2'], inplace=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # convert to lowercase\n",
    "    text = re.sub(r'http\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)  # remove HTML tags\n",
    "    text = re.sub(r'\\W', ' ', text)  # remove non-word characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # remove extra whitespaces\n",
    "    return text\n",
    "\n",
    "# applying text cleaning\n",
    "df['question1'] = df['question1'].apply(clean_text)\n",
    "df['question2'] = df['question2'].apply(clean_text)\n",
    "\n",
    "# tokenization and padding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# batch tokenization function\n",
    "def batch_tokenize_and_encode(questions1, questions2, max_length=128):\n",
    "    encoded_dict = tokenizer.batch_encode_plus(\n",
    "        list(zip(questions1, questions2)),\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoded_dict['input_ids'], encoded_dict['attention_mask']\n",
    "\n",
    "# batch tokenization\n",
    "encoded_inputs, encoded_masks = batch_tokenize_and_encode(df['question1'], df['question2'])\n",
    "print(\"Encoded Inputs:\", encoded_inputs)\n",
    "print(\"Attention Mask:\", encoded_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.head()\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "# checking for missing values\n",
    "print(\"Missing values in the dataset:\\n\", df.isnull().sum())\n",
    "\n",
    "# class distribution\n",
    "sns.countplot(x='is_duplicate', data=df)\n",
    "plt.title(\"Class Distribution (1 = Similar, 0 = Not Similar)\")\n",
    "plt.xlabel(\"Is Duplicate\")\n",
    "plt.ylabel(\"Number of Question Pairs\")\n",
    "plt.show()\n",
    "\n",
    "# statistics\n",
    "print(\"Descriptive statistics of question lengths:\")\n",
    "df['question1_length'] = df['question1'].apply(lambda x: len(str(x).split()))\n",
    "df['question2_length'] = df['question2'].apply(lambda x: len(str(x).split()))\n",
    "print(df[['question1_length', 'question2_length']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-validation split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['is_duplicate'])\n",
    "\n",
    "# checking for overlapping question pairs\n",
    "def check_overlap(train_set, val_set):\n",
    "    train_pairs = set(zip(train_set['question1'], train_set['question2']))\n",
    "    val_pairs = set(zip(val_set['question1'], val_set['question2']))\n",
    "    \n",
    "    overlap = train_pairs.intersection(val_pairs)\n",
    "    return len(overlap) > 0, overlap\n",
    "\n",
    "has_overlap, overlapping_pairs = check_overlap(train_df, val_df)\n",
    "\n",
    "if has_overlap:\n",
    "    print(f\"Overlapping pairs found: {overlapping_pairs}\")\n",
    "    val_df = val_df[~val_df.apply(lambda row: (row['question1'], row['question2']) in overlapping_pairs, axis=1)]\n",
    "    # re-checking sizes after removing overlaps\n",
    "    print(f\"New Validation Set Size: {len(val_df)}\")\n",
    "\n",
    "print(f\"Training Set Size: {len(train_df)}\")\n",
    "print(f\"Validation Set Size: {len(val_df)}\")\n",
    "\n",
    "# datasets for model input\n",
    "def create_dataset(df, max_length=128):\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for _, row in df.iterrows():\n",
    "        q1, q2 = row['question1'], row['question2']\n",
    "        label = row['is_duplicate']\n",
    "        \n",
    "        encoded_inputs, encoded_masks = batch_tokenize_and_encode(q1, q2, max_length)\n",
    "        \n",
    "        input_ids.append(encoded_inputs)\n",
    "        attention_masks.append(encoded_masks)\n",
    "        labels.append(label)\n",
    "    \n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0), torch.tensor(labels)\n",
    "\n",
    "train_inputs, train_masks, train_labels = create_dataset(train_df)\n",
    "val_inputs, val_masks, val_labels = create_dataset(val_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False\n",
    ")\n",
    "\n",
    "# moving model to GPU if available\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "import logging\n",
    "import random\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# dataloader\n",
    "batch_size = 16\n",
    "\n",
    "# training dataloader\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# validation dataloader\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# optimizer and learning rate scheduler setup\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "total_steps = len(train_dataloader) * epochs  # total training steps\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# training loop\n",
    "epochs = 100\n",
    "training_loss_values = []\n",
    "validation_loss_values = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 80  #  epochs to wait before stopping\n",
    "patience_counter = 0  # counter for early stopping\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    logging.info(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    # training phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    training_loss_values.append(avg_train_loss)\n",
    "    logging.info(f\"Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "    # validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            val_loss = outputs.loss\n",
    "            total_val_loss += val_loss.item()\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    validation_loss_values.append(avg_val_loss)\n",
    "    logging.info(f\"Validation loss: {avg_val_loss:.2f}\")\n",
    "    \n",
    "    # early stopping logic\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        # save the model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logging.info(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "predictions, true_labels, original_indices = [], [], []\n",
    "\n",
    "# evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # collecting original indices (dataloader should maintain the order)\n",
    "        original_indices.extend(batch[0].cpu().numpy())  # the first item in the batch corresponds to the original indices\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(b_labels.cpu().numpy())\n",
    "    \n",
    "    # calculating metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    # calculating ROC-AUC score\n",
    "    roc_auc = roc_auc_score(true_labels, [logit[1] for logit in logits.cpu().numpy()])  # Assuming binary classification\n",
    "    \n",
    "    # error analysis\n",
    "    misclassified_indices = [i for i, (true, pred) in enumerate(zip(true_labels, predictions)) if true != pred]\n",
    "    misclassified_examples = [(original_indices[i], true_labels[i], predictions[i]) for i in misclassified_indices]\n",
    "    \n",
    "    return accuracy, precision, recall, f1, roc_auc, misclassified_examples\n",
    "\n",
    "# evaluating on validation set\n",
    "val_accuracy, val_precision, val_recall, val_f1, val_roc_auc, misclassified_examples = evaluate(model, val_dataloader)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Validation Precision: {val_precision:.2f}\")\n",
    "print(f\"Validation Recall: {val_recall:.2f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.2f}\")\n",
    "print(f\"Validation ROC-AUC: {val_roc_auc:.2f}\")\n",
    "\n",
    "# misclassified examples (original index, true label, predicted label)\n",
    "for idx, true_label, pred_label in misclassified_examples:\n",
    "    print(f\"Misclassified Example - Index: {idx}, True Label: {true_label}, Predicted Label: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# displaying some misclassified examples\n",
    "misclassified_idx = [i for i, (pred, true) in enumerate(zip(predictions, true_labels)) if pred != true]\n",
    "for idx in misclassified_idx[:5]:\n",
    "    q1 = val_df.iloc[idx]['question1']\n",
    "    q2 = val_df.iloc[idx]['question2']\n",
    "    print(f\"Q1: {q1}\\nQ2: {q2}\\nPredicted: {predictions[idx]}, Actual: {true_labels[idx]}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# objective function for Optuna\n",
    "def objective(trial):\n",
    "    # hyperparameters to tune\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-4)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
    "    epochs = trial.suggest_int('epochs', 2, 5)\n",
    "    warmup_steps = trial.suggest_int('warmup_steps', 0, 100)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
    "    \n",
    "    # a smaller subset of the training data for the trials\n",
    "    small_train_data = train_data[:int(len(train_data) * 0.1)]  # 10% of the original data\n",
    "    small_val_data = val_data[:int(len(val_data) * 0.1)]  # 10% of the original validation data\n",
    "\n",
    "    # dataloader setup with trial batch size\n",
    "    train_dataloader = DataLoader(small_train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(small_val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "    \n",
    "    # model setup\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=2,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    ).to(device)\n",
    "    \n",
    "    # applying dropout in model if necessary (custom model modification may be required)\n",
    "    model.dropout.p = dropout_rate  # dropout rate setting (this assumes you have a model that allows setting dropout rate)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, eps=1e-8, weight_decay=weight_decay)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    \n",
    "    # training loop with early stopping\n",
    "    best_f1 = 0\n",
    "    patience = 2\n",
    "    patience_counter = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        # validation\n",
    "        accuracy, precision, recall, f1 = evaluate(model, val_dataloader)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # checking for early stopping\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return best_f1  # objective: maximize F1 score\n",
    "\n",
    "# running the Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(f\"Best hyperparameters: {study.best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_path = 'q_quora_test.csv'  # from a separate dataset\n",
    "test_df = pd.read_csv(test_data_path, usecols=['id', 'qid1', 'qid2', 'question1', 'question2'])\n",
    "\n",
    "# cleaning and preprocessing the test data\n",
    "test_df.dropna(subset=['question1', 'question2'], inplace=True)\n",
    "test_df['question1'] = test_df['question1'].apply(clean_text)\n",
    "test_df['question2'] = test_df['question2'].apply(clean_text)\n",
    "\n",
    "test_inputs, test_masks, test_labels = create_dataset(test_df)\n",
    "\n",
    "# dataloader for the test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=16)\n",
    "\n",
    "# evaluating the test set\n",
    "test_accuracy, test_precision, test_recall, test_f1 = evaluate(model, test_dataloader)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Test Precision: {test_precision:.2f}\")\n",
    "print(f\"Test Recall: {test_recall:.2f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# plotting training vs. validation Loss\n",
    "plt.plot(training_loss_values, label=\"Training Loss\")\n",
    "plt.plot(validation_loss_values, label=\"Validation Loss\")\n",
    "plt.title(\"Training vs. Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits  # the raw logits\n",
    "            predictions.extend(logits.softmax(dim=1).cpu().numpy())  # logits to probabilities\n",
    "            true_labels.extend(b_labels.cpu().numpy())\n",
    "    \n",
    "    return true_labels, predictions\n",
    "\n",
    "true_labels, predictions = evaluate(model, test_dataloader)\n",
    "fpr, tpr, _ = roc_curve(true_labels, predictions[:, 1])  # probabilities for the positive class\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
